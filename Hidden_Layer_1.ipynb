{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from numpy.linalg import inv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covariance matrix\n",
    "def sigma(theta,diag):\n",
    "    a = np.array((math.cos(theta),math.sin(theta)))\n",
    "    b = np.array((-math.sin(theta),math.cos(theta)))\n",
    "    V = np.column_stack((a,b))\n",
    "    return V*diag*inv(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating 2D synthetic data\n",
    "NS = 4000 # number of samples\n",
    "\n",
    "# Generating data for Class 0\n",
    "m0 = [0,0] #mean\n",
    "diag0 = [[2,0],[0,1]]\n",
    "theta0 = 0\n",
    "sigma0 = sigma(theta0,diag0) #covariance\n",
    "C0 = np.random.multivariate_normal(m0,sigma0,NS).T\n",
    "x_class0 = C0[0]\n",
    "y_class0 = C0[1]\n",
    "#     return np.random.multivariate_normal(m0,sigma0,NS).T\n",
    "\n",
    "# Generating data for Class 1\n",
    "#Component A\n",
    "m1_A = [-2,1]\n",
    "NS_A = math.ceil(NS/3)\n",
    "diag1_A = [[2,0],[0,1/4]]\n",
    "theta1_A = -(3//4)*math.pi\n",
    "sigmaA = sigma(theta1_A,diag1_A) #covariance\n",
    "A = np.random.multivariate_normal(m1_A,sigmaA,NS_A).T\n",
    "x_compA = A[0]\n",
    "y_compA = A[1]\n",
    "\n",
    "#Component B\n",
    "\n",
    "m1_B = [3,2]\n",
    "NS_B = math.floor(2*(NS/3))\n",
    "diag1_B = [[3,0],[0,1]]\n",
    "theta1_B = (1//4)*math.pi\n",
    "sigmaB = sigma(theta1_B,diag1_B)\n",
    "B = np.random.multivariate_normal(m1_B,sigmaB,NS_B).T\n",
    "x_compB = B[0]\n",
    "y_compB = B[1]\n",
    "\n",
    "\n",
    "#Combining Components A and B to obtain Class 1\n",
    "x_class1 = np.concatenate((x_compA,x_compB))\n",
    "y_class1 = np.concatenate((y_compA,y_compB))\n",
    "\n",
    "C1 = x_class1,y_class1\n",
    "C1 = np.asarray(C1)\n",
    "\n",
    "# print('C0',C0)\n",
    "# print('C1',C1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the two classes\n",
    "plt.plot(x_class0,y_class0,'ro',label=\"class0\")\n",
    "plt.title(\"Scatter plot of Class 0\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_class1,y_class1,'bo',label=\"class1\")\n",
    "plt.title(\"Scatter plot of Class 1\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the generated data into Training,testing and validation in the ratio of 70:20:10 \n",
    "\n",
    "#Splitting the 2 classes individually to keep the datasets balanced\n",
    "# splitting_ratio = [0.7,0.2,0.1]\n",
    "\n",
    "#Splitting Class0 data\n",
    "trainingC0 = C0[:,0:math.floor(0.7*NS/2)]\n",
    "testingC0 = C0[:,math.floor(0.7*NS/2):(math.floor(0.7*NS/2)+math.floor(0.2*NS/2))]\n",
    "validationC0 = C0[:,-(math.floor(0.1*NS/2)):]\n",
    "# print('The size of Class 0 training set is',np.shape(trainingC0))\n",
    "# print('The size of Class 0 testing set is',np.shape(testingC0))\n",
    "# print('The size of Class 0 validation set is',np.shape(validationC0))\n",
    "\n",
    "# Splitting Class1 data\n",
    "trainingC1 = C1[:,0:math.floor(0.7*NS/2)]\n",
    "testingC1 = C1[:,math.floor(0.7*NS/2):(math.floor(0.7*NS/2)+math.floor(0.2*NS/2))]\n",
    "validationC1 = C1[:,-(math.floor(0.1*NS/2)):]\n",
    "# print('The size of Class 1 training set is',np.shape(trainingC1))\n",
    "# print('The size of Class 1 training set is',np.shape(testingC1))\n",
    "# print('The size of Class 1 training set is',np.shape(validationC1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Class 0 and Class 1 data\n",
    "training = np.concatenate([trainingC0, trainingC1], axis=1)\n",
    "testing = np.concatenate([testingC0, testingC1], axis=1)\n",
    "validation = np.concatenate([validationC0, validationC1], axis=1)\n",
    "print('The size of training set is',np.shape(training))\n",
    "print('The size of testing set is',np.shape(testing))\n",
    "print('The size of validation set is',np.shape(validation))\n",
    "\n",
    "y_training = np.append(np.zeros(math.floor(0.7*NS/2)), np.ones(math.floor(0.7*NS/2)))\n",
    "y_test = np.append(np.zeros(math.floor(0.2*NS/2)), np.ones(math.floor(0.2*NS/2)))\n",
    "y_validation = np.append(np.zeros(math.floor(0.1*NS/2)), np.ones(math.floor(0.1*NS/2)))\n",
    "# print('y_training',y_training)\n",
    "print(len(y_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling of the combined data\n",
    "training_shuffled = np.random.permutation(len(y_training))\n",
    "testing_shuffled = np.random.permutation(len(y_test))\n",
    "validation_shuffled = np.random.permutation(len(y_validation))\n",
    "# print('training_shuffled',training_shuffled)\n",
    "\n",
    "training = training[:,training_shuffled]\n",
    "test = testing[:,testing_shuffled]\n",
    "validation = validation[:,validation_shuffled]\n",
    "# print('training',training)\n",
    "\n",
    "y_training = y_training[training_shuffled]\n",
    "y_test = y_test[testing_shuffled]\n",
    "y_validation = y_validation[validation_shuffled]\n",
    "# print('y_training_shuffled',y_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with 2 hidden layers\n",
    "#Defining the parameters of the model\n",
    "learning_rate = 0.01\n",
    "steps = int(NS*0.7)\n",
    "epochs = 60\n",
    "batch_size = 10\n",
    "\n",
    "hidden_layer_1 = 10 # number of neurons in first layer\n",
    "input_size = 2\n",
    "labels = 1\n",
    "\n",
    "#Creating placeholders for input size and labels\n",
    "X = tf.placeholder(\"float\", [None, input_size])\n",
    "Y = tf.placeholder(\"float\", [None, labels])\n",
    "\n",
    "# Creating Weights and Biases\n",
    "weights = {'h1': tf.Variable(tf.random_normal([input_size, hidden_layer_1])),\n",
    "           'out': tf.Variable(tf.random_normal([hidden_layer_1, labels]))\n",
    "}\n",
    "biases = {'b1': tf.Variable(tf.random_normal([hidden_layer_1])),\n",
    "          'out': tf.Variable(tf.random_normal([labels]))\n",
    "}\n",
    "\n",
    "# Neural Network with 1 layer\n",
    "def NN(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #l1 = h1*x+b1\n",
    "    layer_1 = tf.nn.relu(layer_1) #Adding Relu nonlinearity in layer 1\n",
    "    \n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out'] #output = wout*l1+bout\n",
    "    return out_layer\n",
    "\n",
    "#This logits will be passed through softmax layer\n",
    "logits = NN(X)\n",
    "prediction = tf.nn.sigmoid(logits)\n",
    "prediction = tf.cast(tf.greater(prediction, 1/2), tf.float32)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Calculating accuracy with test set\n",
    "correct_pred = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "# Train the model epochs times\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for batch in range(0, steps, batch_size):\n",
    "            batch_x = training[:,batch:batch+batch_size].T\n",
    "            batch_y = y_training[batch:batch+batch_size].reshape([-1,1])\n",
    "            \n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            \n",
    "            \n",
    "            print(\"epoch_number \" + str(epoch) + \", batch: \" + str(batch/batch_size + 1) + \\\n",
    "                  \", loss = \" + \"{:.4f}\".format(loss) + \\\n",
    "                  \", accuracy on training set = \" + \"{:.3f}\".format(100*acc))\n",
    "            \n",
    "\n",
    "    # Calculate accuracy for test data\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test.T,\n",
    "                                      Y: y_test.reshape([-1,1])}))\n",
    "    \n",
    "    print(\"Validation Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: validation.T,\n",
    "                                      Y: y_validation.reshape([-1,1])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
